---
permalink: /
title: "Junteng Liu"
author_profile: true
redirect_from:
  - /about/
  - /about.html
---

I am a first-year Ph.D. candidate at the [HKUST NLP Group](https://nlp.hkust.edu.hk/), [Hong Kong University of Science and Technology (HKUST)](https://hkust.edu.hk/), advised by [Prof. Junxian He](https://jxhe.github.io/). I received my B.Eng. from [Shanghai Jiao Tong University (SJTU)](https://en.sjtu.edu.cn/) in June 2024, where I was also fortunate to work with Prof. Junxian He during my undergraduate studies.

My research lies at the intersection of **natural language processing** and **machine learning**, with a particular focus on understanding and improving the capabilities and reliability of large language models (LLMs) and vision-language models (VLMs).

## Research Interests

- **LLM Reasoning and Reinforcement Learning**: Exploring scalable methods to synthesize verifiable reasoning data and enhance logical reasoning capabilities of large language models through reinforcement learning.
- **Hallucination in Vision-Language Models (VLMs)**: Investigating perception bottlenecks and developing techniques to mitigate hallucination in multimodal tasks such as chart understanding.
- **LLM Truthfulness and Interpretability**: Studying the internal representations of LLMs to understand truthfulness mechanisms and developing interpretability tools to improve model reliability.

## Selected Publications

**[SynLogic: Synthesizing Verifiable Reasoning Data at Scale for Learning Logical Reasoning and Beyond](https://arxiv.org/abs/2505.19641)**  
**Junteng Liu**, Yuanxiang Fan, Zhuo Jiang, Han Ding, Yongyi Hu, Chi Zhang, Yiqi Shi, Shitong Weng, Aili Chen, Shiqi Chen, Yunan Huang, Mozhi Zhang, Pengyu Zhao, Junjie Yan, Junxian He  
*Preprint, 2025*

**[On the Perception Bottleneck of VLMs for Chart Understanding](https://arxiv.org/abs/2503.18435)**  
**Junteng Liu**, Weihao Zeng, Xiwen Zhang, Yijun Wang, Zifei Shan, Junxian He  
*Preprint, 2025*

**[On the Universal Truthfulness Hyperplane Inside LLMs](https://arxiv.org/abs/2407.08582)**  
**Junteng Liu**, Shiqi Chen, Yu Cheng, Junxian He  
*EMNLP 2024*

**[In-Context Sharpness as Alerts: An Inner Representation Perspective for Hallucination Mitigation](https://arxiv.org/abs/2403.01548)**  
Shiqi Chen, Miao Xiong, **Junteng Liu**, Zhengxuan Wu, Teng Xiao, Siyang Gao, Junxian He  
*ICML 2024*

**[C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models](https://arxiv.org/abs/2305.08322)**  
Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, **Junteng Liu**, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Yao Fu, Maosong Sun, Junxian He  
*NeurIPS 2023*

**[Composing Parameter-Efficient Modules with Arithmetic Operations](https://arxiv.org/abs/2306.14870)**  
Jinghan Zhang, Shiqi Chen, **Junteng Liu**, Junxian He  
*NeurIPS 2023*

## Education

- **Ph.D. in Computer Science** (2024–Present)  
  Hong Kong University of Science and Technology (HKUST)  
  *Advisor: [Prof. Junxian He](https://jxhe.github.io/)*

- **B.Eng.** (2020–2024)  
  Shanghai Jiao Tong University (SJTU)  
  *Recipient of Zhiyuan Honor Scholarship*

## Experience

- **Research Intern**, MiniMax (February 2025 – Present)  
  Working on large language models and reasoning.

- **Research Intern**, Tencent WXG (June 2024 – September 2024)  
  *Advisor: Zifei Shan* — Research on vision-language models.

- **Research Intern**, Shanghai AI Lab (June 2023 – December 2023)  
  *Advisor: Prof. Yu Cheng* — Research on LLM truthfulness and interpretability.

## Contact

- **Email**: [jliugi@connect.ust.hk](mailto:jliugi@connect.ust.hk)
- **GitHub**: [Vicent0205](https://github.com/Vicent0205)
- **Google Scholar**: [Junteng Liu](https://scholar.google.com/citations?hl=en&user=tbK9jl4AAAAJ&view_op=list_works&sortby=pubdate)
- **X (Twitter)**: [@junteng88716710](https://twitter.com/junteng88716710)
